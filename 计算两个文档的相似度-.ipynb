{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be493d62",
   "metadata": {},
   "source": [
    "## 课程图谱相关实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0bd3f5",
   "metadata": {},
   "source": [
    "### 如何计算两个文档的相似度三 https://mp.weixin.qq.com/s/ZnGT3HzJ0BHc9td8GFbgqw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "675a4613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Writing II: Rhetorical Composing', 'Genetics and Society: A Course for Educators', 'General Game Playing', 'Genes and the Human Condition (From Behavior to Biotechnology)', 'A Brief History of Humankind', 'New Models of Business in Society', 'Analyse Numérique pour Ingénieurs', 'Evolution: A Course for Educators', 'Coding the Matrix: Linear Algebra through Computer Science Applications', 'The Dynamic Earth: A Course for Educators']\n"
     ]
    }
   ],
   "source": [
    "#1、数据准备\n",
    "#打开Python, 加载这份数据：coursera_corpus 需要下载到和ipnb同目录\n",
    "#公众号 AINLP 百度网盘链接: http://t.cn/RhjgPkv，密码: oppc\n",
    "courses = [line.strip() for line in open('coursera_corpus',encoding='utf-8')]\n",
    "courses_name = [course.split('\\t')[0] for course in courses]\n",
    "print (courses_name[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e133d211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2、引入NLTK(NTLK是著名的Python自然语言处理工具包，但是主要针对的是英文处理)\n",
    "import nltk\n",
    "\n",
    "#下载NLTK官方提供的相关语料\n",
    "#nltk.download()\n",
    "#下载失败可手动下载 网盘链接：https://pan.baidu.com/s/1MgP5qLYZr2xw0Cpf54Vq4w  提取码：1024\n",
    "#C:\\Users\\zichen\\AppData\\Roaming\\nltk_data \n",
    "#语料下载完毕后，NLTK在你的电脑上才真正达到可用的状态，可以测试一下布朗语料库：\n",
    "from nltk.corpus import brown\n",
    "brown.readme()\n",
    "\n",
    "brown.words()[0:10]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13cf7d9f",
   "metadata": {},
   "source": [
    "TimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。\n",
    "https://blog.csdn.net/xiaotao_1/article/details/79834092\n",
    "[如有改报错，建议手动下载nltk_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167638b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23267df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e55fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['writing', 'ii:', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'you', 'in', 'a', 'series', 'of', 'interactive', 'reading,', 'research,', 'and', 'composing', 'activities', 'along', 'with', 'assignments', 'designed', 'to', 'help', 'you', 'become', 'more', 'effective', 'consumers', 'and', 'producers', 'of', 'alphabetic,', 'visual', 'and', 'multimodal', 'texts.', 'join', 'us', 'to', 'become', 'more', 'effective', 'writers...', 'and', 'better', 'citizens.', 'rhetorical', 'composing', 'is', 'a', 'course', 'where', 'writers', 'exchange', 'words,', 'ideas,', 'talents,', 'and', 'support.', 'you', 'will', 'be', 'introduced', 'to', 'a', 'variety', 'of', 'rhetorical', 'concepts—that', 'is,', 'ideas', 'and', 'techniques', 'to', 'inform', 'and', 'persuade', 'audiences—that', 'will', 'help', 'you', 'become', 'a', 'more', 'effective', 'consumer', 'and', 'producer', 'of', 'written,', 'visual,', 'and', 'multimodal', 'texts.', 'the', 'class', 'includes', 'short', 'videos,', 'demonstrations,', 'and', 'activities.', 'we', 'envision', 'rhetorical', 'composing', 'as', 'a', 'learning', 'community', 'that', 'includes', 'both', 'those', 'enrolled', 'in', 'this', 'course', 'and', 'the', 'instructors.', 'we', 'bring', 'our', 'expertise', 'in', 'writing,', 'rhetoric', 'and', 'course', 'design,', 'and', 'we', 'have', 'designed', 'the', 'assignments', 'and', 'course', 'infrastructure', 'to', 'help', 'you', 'share', 'your', 'experiences', 'as', 'writers,', 'students,', 'and', 'professionals', 'with', 'each', 'other', 'and', 'with', 'us.', 'these', 'collaborations', 'are', 'facilitated', 'through', 'wex,', 'the', 'writers', 'exchange,', 'a', 'place', 'where', 'you', 'will', 'exchange', 'your', 'work', 'and', 'feedback']\n"
     ]
    }
   ],
   "source": [
    "#处理课程数据：对文档的单词小写化的话，将得到如下的结果：\n",
    "\n",
    "texts_lower = [[word for word in document.lower().split()] for document in courses]\n",
    "\n",
    "print (texts_lower[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c30d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['writing', 'ii', ':', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'you', 'in', 'a', 'series', 'of', 'interactive', 'reading', ',', 'research', ',', 'and', 'composing', 'activities', 'along', 'with', 'assignments', 'designed', 'to', 'help', 'you', 'become', 'more', 'effective', 'consumers', 'and', 'producers', 'of', 'alphabetic', ',', 'visual', 'and', 'multimodal', 'texts', '.', 'join', 'us', 'to', 'become', 'more', 'effective', 'writers', '...', 'and', 'better', 'citizens', '.', 'rhetorical', 'composing', 'is', 'a', 'course', 'where', 'writers', 'exchange', 'words', ',', 'ideas', ',', 'talents', ',', 'and', 'support', '.', 'you', 'will', 'be', 'introduced', 'to', 'a', 'variety', 'of', 'rhetorical', 'concepts—that', 'is', ',', 'ideas', 'and', 'techniques', 'to', 'inform', 'and', 'persuade', 'audiences—that', 'will', 'help', 'you', 'become', 'a', 'more', 'effective', 'consumer', 'and', 'producer', 'of', 'written', ',', 'visual', ',', 'and', 'multimodal', 'texts', '.', 'the', 'class', 'includes', 'short', 'videos', ',', 'demonstrations', ',', 'and', 'activities', '.', 'we', 'envision', 'rhetorical', 'composing', 'as', 'a', 'learning', 'community', 'that', 'includes', 'both', 'those', 'enrolled', 'in', 'this', 'course', 'and', 'the', 'instructors', '.', 'we', 'bring', 'our', 'expertise', 'in', 'writing', ',', 'rhetoric', 'and', 'course', 'design', ',', 'and', 'we', 'have', 'designed', 'the', 'assignments', 'and', 'course', 'infrastructure', 'to', 'help', 'you', 'share', 'your', 'experiences', 'as', 'writers', ',', 'students', ',', 'and', 'professionals', 'with', 'each', 'other', 'and', 'with', 'us', '.', 'these', 'collaborations', 'are', 'facilitated', 'through', 'wex', ',', 'the', 'writers', 'exchange', ',', 'a', 'place', 'where', 'you', 'will', 'exchange', 'your', 'work', 'and', 'feedback']\n"
     ]
    }
   ],
   "source": [
    "#注意其中很多标点符号和单词是没有分离的，所以我们引入nltk的word_tokenize函数，并处理相应的数据：\n",
    "#import h5py\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texts_tokenized = [[word.lower() for word in word_tokenize(document)] for document in courses]\n",
    "\n",
    "print (texts_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482bfb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对课程的英文数据进行tokenize之后，需要去停用词，NLTK提供了一份英文停用词数据：\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "print (english_stopwords)\n",
    "\n",
    "len(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "831cef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['writing', 'ii', ':', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'series', 'interactive', 'reading', ',', 'research', ',', 'composing', 'activities', 'along', 'assignments', 'designed', 'help', 'become', 'effective', 'consumers', 'producers', 'alphabetic', ',', 'visual', 'multimodal', 'texts', '.', 'join', 'us', 'become', 'effective', 'writers', '...', 'better', 'citizens', '.', 'rhetorical', 'composing', 'course', 'writers', 'exchange', 'words', ',', 'ideas', ',', 'talents', ',', 'support', '.', 'introduced', 'variety', 'rhetorical', 'concepts—that', ',', 'ideas', 'techniques', 'inform', 'persuade', 'audiences—that', 'help', 'become', 'effective', 'consumer', 'producer', 'written', ',', 'visual', ',', 'multimodal', 'texts', '.', 'class', 'includes', 'short', 'videos', ',', 'demonstrations', ',', 'activities', '.', 'envision', 'rhetorical', 'composing', 'learning', 'community', 'includes', 'enrolled', 'course', 'instructors', '.', 'bring', 'expertise', 'writing', ',', 'rhetoric', 'course', 'design', ',', 'designed', 'assignments', 'course', 'infrastructure', 'help', 'share', 'experiences', 'writers', ',', 'students', ',', 'professionals', 'us', '.', 'collaborations', 'facilitated', 'wex', ',', 'writers', 'exchange', ',', 'place', 'exchange', 'work', 'feedback']\n"
     ]
    }
   ],
   "source": [
    "#总计127个停用词，我们首先过滤课程语料中的停用词：\n",
    "\n",
    "texts_filtered_stopwords = [[word for word in document if not word in english_stopwords] for document in texts_tokenized]\n",
    "\n",
    "print (texts_filtered_stopwords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e830d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['writing', 'ii', 'rhetorical', 'composing', 'rhetorical', 'composing', 'engages', 'series', 'interactive', 'reading', 'research', 'composing', 'activities', 'along', 'assignments', 'designed', 'help', 'become', 'effective', 'consumers', 'producers', 'alphabetic', 'visual', 'multimodal', 'texts', 'join', 'us', 'become', 'effective', 'writers', '...', 'better', 'citizens', 'rhetorical', 'composing', 'course', 'writers', 'exchange', 'words', 'ideas', 'talents', 'support', 'introduced', 'variety', 'rhetorical', 'concepts—that', 'ideas', 'techniques', 'inform', 'persuade', 'audiences—that', 'help', 'become', 'effective', 'consumer', 'producer', 'written', 'visual', 'multimodal', 'texts', 'class', 'includes', 'short', 'videos', 'demonstrations', 'activities', 'envision', 'rhetorical', 'composing', 'learning', 'community', 'includes', 'enrolled', 'course', 'instructors', 'bring', 'expertise', 'writing', 'rhetoric', 'course', 'design', 'designed', 'assignments', 'course', 'infrastructure', 'help', 'share', 'experiences', 'writers', 'students', 'professionals', 'us', 'collaborations', 'facilitated', 'wex', 'writers', 'exchange', 'place', 'exchange', 'work', 'feedback']\n"
     ]
    }
   ],
   "source": [
    "#停用词被过滤了，不过发现标点符号还在，这个好办，我们首先定义一个标点符号list:\n",
    "\n",
    "english_punctuations = [',','.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', \"#\", '$', '%']\n",
    "                        \n",
    "#然后过滤这些标点符号：\n",
    "texts_filtered = [[word for word in document if not word in english_punctuations] for document in texts_filtered_stopwords]\n",
    "print (texts_filtered[0])                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4727775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['writ', 'ii', 'rhet', 'compos', 'rhet', 'compos', 'eng', 'sery', 'interact', 'read', 'research', 'compos', 'act', 'along', 'assign', 'design', 'help', 'becom', 'effect', 'consum', 'produc', 'alphabet', 'vis', 'multimod', 'text', 'join', 'us', 'becom', 'effect', 'writ', '...', 'bet', 'cit', 'rhet', 'compos', 'cours', 'writ', 'exchang', 'word', 'idea', 'tal', 'support', 'introduc', 'vary', 'rhet', 'concepts—that', 'idea', 'techn', 'inform', 'persuad', 'audiences—that', 'help', 'becom', 'effect', 'consum', 'produc', 'writ', 'vis', 'multimod', 'text', 'class', 'includ', 'short', 'video', 'demonst', 'act', 'envid', 'rhet', 'compos', 'learn', 'commun', 'includ', 'enrol', 'cours', 'instruct', 'bring', 'expert', 'writ', 'rhet', 'cours', 'design', 'design', 'assign', 'cours', 'infrastruct', 'help', 'shar', 'expery', 'writ', 'stud', 'profess', 'us', 'collab', 'facilit', 'wex', 'writ', 'exchang', 'plac', 'exchang', 'work', 'feedback']\n"
     ]
    }
   ],
   "source": [
    "#更进一步，我们对这些英文单词词干化（Stemming)，\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "st.stem('stemmed')\n",
    "\n",
    "#让我们调用这个接口来处理上面的课程数据:\n",
    "\n",
    "texts_stemmed = [[st.stem(word) for word in docment] for docment in texts_filtered]\n",
    "\n",
    "print (texts_stemmed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605657bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#在引入gensim前，需要去掉在整个语料库中出现次数为1的低频词，测试了一下，不去掉的话对效果有些影响：\n",
    "\n",
    "all_stems = sum(texts_stemmed, [])\n",
    "\n",
    "stems_once = set(stem for stem in set(all_stems) if all_stems.count(stem) == 1)\n",
    "\n",
    "texts = [[stem for stem in text if stem not in stems_once] for text in texts_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90b8c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 10:20:42,485 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2022-10-23 10:20:42,545 : INFO : built Dictionary(2974 unique tokens: ['...', 'act', 'along', 'assign', 'becom']...) from 379 documents (total 47550 corpus positions)\n",
      "2022-10-23 10:20:42,546 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(2974 unique tokens: ['...', 'act', 'along', 'assign', 'becom']...) from 379 documents (total 47550 corpus positions)\", 'datetime': '2022-10-23T10:20:42.546432', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'created'}\n",
      "2022-10-23 10:20:42,646 : INFO : collecting document frequencies\n",
      "2022-10-23 10:20:42,647 : INFO : PROGRESS: processing document #0\n",
      "2022-10-23 10:20:42,665 : INFO : TfidfModel lifecycle event {'msg': 'calculated IDF weights for 379 documents and 2974 features (28933 matrix non-zeros)', 'datetime': '2022-10-23T10:20:42.665115', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'initialize'}\n"
     ]
    }
   ],
   "source": [
    "#3、引入gensim\n",
    "\n",
    "#有了上述的预处理，我们就可以引入gensim，并快速的做课程相似度的实验了。\n",
    "#以下会快速的过一遍流程，具体的可以参考上一节的详细描述。\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25ac394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 10:20:52,412 : INFO : using serial LSI version on this node\n",
      "2022-10-23 10:20:52,413 : INFO : updating model with new documents\n",
      "2022-10-23 10:20:52,414 : INFO : preparing a new chunk of documents\n",
      "2022-10-23 10:20:52,422 : INFO : using 100 extra samples and 2 power iterations\n",
      "2022-10-23 10:20:52,424 : INFO : 1st phase: constructing (2974, 110) action matrix\n",
      "2022-10-23 10:20:52,434 : INFO : orthonormalizing (2974, 110) action matrix\n",
      "2022-10-23 10:20:52,715 : INFO : 2nd phase: running dense svd on (110, 379) matrix\n",
      "2022-10-23 10:20:52,728 : INFO : computing the final decomposition\n",
      "2022-10-23 10:20:52,729 : INFO : keeping 10 factors (discarding 51.296% of energy spectrum)\n",
      "2022-10-23 10:20:52,730 : INFO : processed documents up to #379\n",
      "2022-10-23 10:20:52,733 : INFO : topic #0(158.197): -0.702*\"nbsp\" + -0.319*\"cours\" + -0.224*\"teach\" + -0.178*\"learn\" + -0.141*\"us\" + -0.135*\"’\" + -0.125*\"stud\" + -0.088*\"understand\" + -0.085*\"develop\" + -0.083*\"pract\"\n",
      "2022-10-23 10:20:52,734 : INFO : topic #1(92.118): 0.581*\"nbsp\" + -0.338*\"cours\" + 0.281*\"de\" + -0.253*\"learn\" + 0.190*\"la\" + -0.169*\"us\" + -0.132*\"stud\" + 0.109*\"en\" + -0.092*\"understand\" + -0.089*\"design\"\n",
      "2022-10-23 10:20:52,735 : INFO : topic #2(86.857): -0.561*\"de\" + -0.399*\"la\" + 0.329*\"nbsp\" + -0.245*\"en\" + -0.200*\"un\" + -0.159*\"el\" + -0.138*\"que\" + -0.127*\"cours\" + -0.125*\"program\" + -0.119*\"par\"\n",
      "2022-10-23 10:20:52,738 : INFO : topic #3(74.714): -0.689*\"teach\" + -0.252*\"’\" + -0.208*\"feedback\" + -0.165*\"coach\" + 0.164*\"nbsp\" + 0.155*\"cours\" + -0.123*\"pract\" + -0.103*\"classroom\" + 0.101*\"comput\" + 0.098*\"program\"\n",
      "2022-10-23 10:20:52,741 : INFO : topic #4(57.183): -0.465*\"program\" + -0.313*\"learn\" + 0.309*\"heal\" + 0.225*\"cours\" + -0.213*\"comput\" + 0.181*\"’\" + -0.178*\"langu\" + -0.174*\"us\" + 0.148*\"glob\" + 0.123*\"chang\"\n",
      "2022-10-23 10:20:52,742 : INFO : LsiModel lifecycle event {'msg': 'trained LsiModel(num_terms=2974, num_topics=10, decay=1.0, chunksize=20000) in 0.33s', 'datetime': '2022-10-23T10:20:52.742128', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'created'}\n",
      "2022-10-23 10:20:52,743 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2022-10-23 10:20:52,768 : INFO : creating matrix with 379 documents and 10 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning\n"
     ]
    }
   ],
   "source": [
    "#这里我们拍脑门决定训练topic数量为10的LSI模型：\n",
    "\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=10)\n",
    "\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "\n",
    "#基于LSI模型的课程索引建立完毕，我们以Andrew Ng教授的机器学习公开课为例，这门课程在我们的coursera_corpus文件的第211行，也就是：\n",
    "\n",
    "print (courses_name[210]) #Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0775a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -8.0860236150747), (1, -10.000493814517688), (2, -3.422787869238903), (3, -0.7098358839738389), (4, -9.039592694916744), (5, 6.263793562439042), (6, 3.829285425276287), (7, -10.152084471174247), (8, -1.8940477674503782), (9, 9.002940035933019)]\n"
     ]
    }
   ],
   "source": [
    "#现在我们就可以通过lsi模型将这门课程映射到10个topic主题模型空间上，然后和其他课程计算相似度：\n",
    "\n",
    "ml_course = texts[210]\n",
    "ml_bow = dictionary.doc2bow(ml_course)\n",
    "ml_lsi = lsi[ml_bow]\n",
    "\n",
    "print (ml_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccbd3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[ml_lsi]\n",
    "sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "686a7cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(210, 1.0), (174, 0.98646545), (203, 0.9395967), (157, 0.93329096), (63, 0.9240569), (289, 0.913386), (208, 0.8959285), (212, 0.8818484), (189, 0.8661856), (274, 0.8349765)]\n"
     ]
    }
   ],
   "source": [
    "#取按相似度排序的前10门课程：\n",
    "print (sort_sims[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d16d69dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning\n"
     ]
    }
   ],
   "source": [
    "#第一门课程是它自己:Machine Learning\n",
    "print (courses_name[210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4377dff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning\n"
     ]
    }
   ],
   "source": [
    "#第二门课是Coursera上另一位大牛Pedro Domingos机器学习公开课Machine Learning\n",
    "print (courses_name[174])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c3198e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Networks for Machine Learning\n"
     ]
    }
   ],
   "source": [
    "#第三门课是另一位超级大牛Geoffrey Hinton的神经网络公开课，Neural Networks for Machine Learning\n",
    "print (courses_name[203])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d67e1823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emerging Trends & Technologies in the Virtual K-12 Classroom\n"
     ]
    }
   ],
   "source": [
    "#第四门课是Emerging Trends & Technologies in the Virtual K-12 Classroom【和原文不太一样】\n",
    "print (courses_name[157])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc042e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ebb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
