{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888201fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPE+WordPiece代码实践：https://zhuanlan.zhihu.com/p/673692328",
    "#BPE+WordPiece代码实践解析：https://zhuanlan.zhihu.com/p/651430181"
     
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad4382",
   "metadata": {},
   "source": [
    "### 1.BPE分词训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载语料库\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79751869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预分词器\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# initial pre tokenize function\n",
    "gpt2_tokenizer=AutoTokenizer.from_pretrained('gpt2')\n",
    "pre_tokenize_function=gpt2_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str\n",
    "# 预分词 pre tokenize\n",
    "pre_tokenized_corpus=[pre_tokenize_function(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计词频\n",
    "from collections import defaultdict\n",
    "\n",
    "word2count=defaultdict(int)\n",
    "for split_text in pre_tokenized_corpus:\n",
    "    for word, _ in split_text:\n",
    "        word2count[word]+=1 \n",
    "        \n",
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8eab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 构建由字(character)组成的初始词表\n",
    "vocabs=set()\n",
    "\n",
    "for word in word2count:\n",
    "    vocabs.update(list(word))\n",
    "    \n",
    "vocabs=list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837114ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将特殊符号添加到初始词表中\n",
    "vocabs=[\"<|endoftext|>\"]+vocabs.copy()\n",
    "# 基于初始词表对整词进行切分\n",
    "word2splits={word: [c for c in word] for word in word2count}\n",
    "\n",
    "# 统计相邻两个pair的词频\n",
    "def _compute_pair2score(word2split,word2count):\n",
    "    pair2count=defaultdict(int)\n",
    "    for word,word_count in word2count.items():\n",
    "        split=word2split[word]\n",
    "        if len(split)==1:\n",
    "            continue\n",
    "        for i in range(len(split)-1):\n",
    "            pair=(split[i],split[i+1])\n",
    "            pair2count[pair]+=word_count\n",
    "    return pair2count\n",
    "\n",
    "pair2count=_compute_pair2score(word2split,word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97878c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回最高频的pair\n",
    "def _compute_most_score_pair(pair2count):\n",
    "    best_pair=None\n",
    "    max_freq=None\n",
    "    for pair,pair_count in pair2count.items():\n",
    "        if max_freq is None or max_freq<pair_count:\n",
    "            best_pair=pair\n",
    "            max_freq=pair_count\n",
    "    return best_pair\n",
    "\n",
    "best_pair=_compute_most_score_pair(pair2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并最高频的pair，并将其添加到vocabs中\n",
    "merge_rules=[]\n",
    "best_pair=_compute_most_score_pair(pair2count)\n",
    "vocabs.append(best_pair[0]+best_pair[1])\n",
    "merge_rules.append(best_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91260b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用更新后的vocabs重新对词进行切分\n",
    "# 因为每次只合并一个pair，可以通用merge_rules从旧的word2split去更新新的word2split\n",
    "def _merge_pair(a,b,word2splits):\n",
    "    new_word2splits=dict()\n",
    "    for word,split in word2splits.items():\n",
    "        if len(split)==1:\n",
    "            new_word2splits[word]=split\n",
    "        i=0\n",
    "        while i<len(split)-1:\n",
    "            if split[i]==a and split[i+1]==b:\n",
    "                split=split[:i]+[a+b]+split[i+2:]\n",
    "            else:\n",
    "                i+=1\n",
    "        new_word2splits[word]=split \n",
    "        \n",
    "    return new_word2splits\n",
    "\n",
    "new_word2splits=_merge_pair(best_pair[0],best_pair[1],word2splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ad0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新词表直到达到指定的大小\n",
    "vocab_size=50\n",
    "\n",
    "while(len(vocabs))<vocab_size:\n",
    "    pair2count=_compute_pair2score(word2split=word2splits,word2count=word2count)\n",
    "    best_pair=_compute_most_score_pair(pair2count)\n",
    "    vocabs.append(best_pair[0]+best_pair[1])\n",
    "    merge_rules.append(best_pair)\n",
    "    word2splits=_merge_pair(best_pair[0],best_pair[1],word2splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07015e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd749f93",
   "metadata": {},
   "source": [
    "### 2.BPE分词推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#给定一个句子，将其切分成一个token序列。实现步骤为：1）进行预分词pre-tokenize。2）将词切分成字符级别。3）利用合并规则进行合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def tokenize(text:str) -> List[str]:\n",
    "    # pre tokenize\n",
    "    words=[word for word,_ in pre_tokenize_function(text)]\n",
    "    \n",
    "    # split into char level\n",
    "    splits=[[c for c in word] for word in words]\n",
    "    \n",
    "    # apply merge rules\n",
    "    for merge_rule in merge_rules:\n",
    "        for index,split in enumerate(splits):\n",
    "            i=0\n",
    "            while i<len(split)-1:\n",
    "                if split[i]==merge_rule[0] and split[i+1]==merge_rule[1]:\n",
    "                    split=split[:i]+[\"\".join(merge_rule)]+split[i+2:]\n",
    "                else:\n",
    "                    i+=1\n",
    "            splits[index]=split\n",
    "            \n",
    "    return sum(splits,[])  #将嵌套数组进行合并\n",
    "\n",
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488b7a0",
   "metadata": {},
   "source": [
    "#  #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92a6b7",
   "metadata": {},
   "source": [
    "### 3.WordPiece分词训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a322d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# 准备预料\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# init pre tokenize function\n",
    "bert_tokenizer=AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "pre_tokenize_function=bert_tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str\n",
    "\n",
    "# 预分词\n",
    "pre_tokenized_corpus=[pre_tokenize_function(text) for text in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "# 统计词频\n",
    "word2count=defaultdict(int)\n",
    "\n",
    "for split_text in pre_tokenized_corpus:\n",
    "    for word,_ in split_text:\n",
    "        word2count[word]+=1 \n",
    "        \n",
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将特殊符号添加到初始词表中\n",
    "vocabs=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]+vocabs.copy()\n",
    "\n",
    "# 构建由character组成的初始词表\n",
    "# 注意这里每个词（word）除了第一个字符，后面的字符都需要添加\"##\"特殊符号\n",
    "vocab_set=set()\n",
    "for word in word2count:\n",
    "    vocab_set.add(word[0])\n",
    "    vocab_set.update([\"##\"+c for c in word[1:]])\n",
    "    \n",
    "vocabs=list(vocab_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d592e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于初始词表对整词进行切分\n",
    "word2splits={word:[word[0]]+['##'+c for c in word[1:]] for word in word2count}\n",
    "\n",
    "# 统计相邻两个pair的互信息\n",
    "def _compute_pair2score(word2splits,word2count):\n",
    "    vocab2count=defaultdict(int)\n",
    "    pair2count=defaultdict(int)\n",
    "    for word,word_count in word2count.items():\n",
    "        splits=word2splits[word]\n",
    "        if len(splits)==1:\n",
    "            vocab2count[splits[0]]+=word_count\n",
    "        for i in range(len(splits)-1):\n",
    "            pair=(splits[i],splits[i+1])\n",
    "            vocab2count[splits[i]]+=word_count\n",
    "            pair2count[pair]+=word_count\n",
    "        vocab2count[splits[-1]]+=word_count # 每个splits的最后一个在for循环中没有统计到，需要单独添加\n",
    "        \n",
    "    scores={\n",
    "        pair: freq/(vocab2count[pair[0]]*vocab2count[pair[1]]) for pair,freq in pair2count.items()\n",
    "    }\n",
    "    \n",
    "    return scores\n",
    "\n",
    "pair2score=_compute_pair2score(word2splits,word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88788ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63277f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 返回最高互信息的pair\n",
    "def _compute_most_score_pair(pair2score):\n",
    "    best_pair=None\n",
    "    best_score=None \n",
    "    for pair, score in pair2score.items():\n",
    "        if best_score is None or best_score<score:\n",
    "            best_pair=pair\n",
    "            best_score=score \n",
    "            \n",
    "    return best_pair\n",
    "\n",
    "best_pair=_compute_most_score_pair(pair2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc393a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并最高互信息的pair，并将其添加至词表中\n",
    "vocabs.append(best_pair[0]+best_pair[1][2:] if best_pair[1].startswith('##') else best_pair[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc25475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用更新后的词表重新对word2count进行切分\n",
    "def _merge_pair(a,b,word2splits):\n",
    "    new_word2splits=dict()\n",
    "    for word,split in word2splits.items():\n",
    "        if len(split)==1: \n",
    "            new_word2splits[word]=split\n",
    "            continue\n",
    "        i=0\n",
    "        while(i<len(split)-1):\n",
    "            if split[i]==a and split[i+1]==b:\n",
    "                merge=a+b[2:] if b.startswith('##') else a+b \n",
    "                split=split[:i]+[merge]+split[i+2:]\n",
    "            else:\n",
    "                i+=1\n",
    "        new_word2splits[word]=split \n",
    "        \n",
    "    return new_word2splits\n",
    "\n",
    "new_word2splits=_merge_pair(best_pair[0],best_pair[1],word2splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7835cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新词表直到达到指定的大小\n",
    "vocab_size=70\n",
    "\n",
    "while len(vocabs)<vocab_size:\n",
    "    pair2score=_compute_pair2score(word2splits,word2count)\n",
    "    best_pair=_compute_most_score_pair(pair2score)\n",
    "    word2splits=_merge_pair(best_pair[0],best_pair[1],word2splits)\n",
    "    new_token=best_pair[0]+best_pair[1][2:] if best_pair[1].startswith('##') else best_pair[1]\n",
    "    vocabs.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa69bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0871f65",
   "metadata": {},
   "source": [
    "### 4.WordPiece的推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffcc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_word(word):\n",
    "    tokens=[]\n",
    "    while len(word)>0:\n",
    "        i=len(word)\n",
    "        while i>0 and word[:i] not in vocabs:\n",
    "            i-=1\n",
    "        if i==0:\n",
    "            return [['UNK']]\n",
    "        tokens.append(word[:i])\n",
    "        word=word[i:]\n",
    "        if len(word)>0:\n",
    "            word=f\"##{word}\"\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "def tokenize(text):\n",
    "    words=[word for word,_ in pre_tokenize_function(text)]\n",
    "    encoded_words=[_encode_word(word) for word in words]\n",
    "    \n",
    "    return sum(encoded_words,[])\n",
    "\n",
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72945f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a722d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40538d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0920e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24507f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
